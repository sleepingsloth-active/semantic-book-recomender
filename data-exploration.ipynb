{
 "cells": [
  {
   "cell_type": "code",
   "id": "7216105c47adb311",
   "metadata": {},
   "source": [
    "\n",
    "import kagglehub \n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"dylanjcastillo/7k-books-with-metadata\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f67fb13",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_to_dataset = Path(\"C:/Users/Lenovo/.cache/kagglehub/datasets/dylanjcastillo/7k-books-with-metadata/versions/3\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a30ef14d148c9598",
   "metadata": {},
   "source": [
    "import pandas as pd # type: ignore\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5968ce8b7729d44",
   "metadata": {},
   "source": [
    "books=pd.read_csv(f\"{path}/books.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1880373b8fd05537",
   "metadata": {},
   "source": [
    "books"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d10130a295f1961e",
   "metadata": {},
   "source": [
    "import seaborn as sns # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71497d132c64da96",
   "metadata": {},
   "source": [
    "ax=plt.axes()\n",
    "sns.heatmap(books.isna().transpose(), cbar=False, cmap=\"YlGnBu\")\n",
    "\n",
    "plt.xlabel(\"columns\")\n",
    "plt.ylabel(\"missing values\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb8a979d75d793cb",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "books[\"missing_desription\"]=np.where(books[\"description\"].isna(),1,0)\n",
    "books[\"age_of_books\"]=2024-books[\"published_year\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6fe82d8b8f6a403",
   "metadata": {},
   "source": [
    "#create a list of subsets from colums we want from our book data set\n",
    "columns_of_interest=[\"num_pages\",\"age_of_books\",\"missing_desription\",\"average_rating\"]\n",
    "correlation_matrix=books[columns_of_interest].corr(method=\"spearman\")\n",
    "#spearman for non continous value\n",
    "#convert into seabon heat map\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.figure(figsize=(8,6))\n",
    "heatmap=sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\",cbar_kws={\"label\": \"sperman correlation\"})\n",
    "heatmap.set_title(\"Correlation heatmap\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bc1fd2412f0e877",
   "metadata": {},
   "source": [
    "#we are going to delete missing values so first thing first we gonna see how much /many of book will it cos\n",
    "#conditional logic using pandas to find books that are going to have missing description,missing pages and missing average rating,missing publish year\n",
    "missing_info_books = books[\n",
    "    ~(books[\"description\"].isna()) &\n",
    "    ~(books[\"num_pages\"].isna())  &\n",
    "    ~(books[\"average_rating\"].isna())  &\n",
    "    ~(books[\"published_year\"].isna())\n",
    "]\n",
    "missing_info_books"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "630a6ad52f1b9f7d",
   "metadata": {},
   "source": [
    "missing_info_books[\"categories\"].value_counts()\n",
    "missing_info_books[\"categories\"].value_counts().reset_index().sort_values(by=\"count\", ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "460cf1be812db1b9",
   "metadata": {},
   "source": [
    "missing_info_books\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a54a8a5774d4748f",
   "metadata": {},
   "source": [
    "#This keeps the word count where there's text and leaves NaN where there's no description.\n",
    "#applying a conditional transformation:\n",
    "books[\"words_in_description\"] = books[\"description\"].apply(\n",
    "    lambda x: len(x.split()) if isinstance(x, str) else np.nan\n",
    ")\n",
    "\n",
    "books\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f963e2c1ec0035e",
   "metadata": {},
   "source": [
    "#create an arbitrary cut point so a relatively straight forward way of working out where this cut off will be\n",
    "#how many words a description needs to have in order to be meaningful is just come up with some cut_offs come up with some bins or how many words the description needs to have,inspect the descriptions that have those number of words\n",
    "#create some pandas code to filter the pandas data frame so that we have all of those books where the words in description are between 1 and 4 to start ,so there's going to be our smallest group and we really just want to return the descriptions\n",
    "# Step 1: Fill missing descriptions with empty string (so we can safely count words)\n",
    "# Make a copy to safely work on it without warnings\n",
    "missing_info_books = missing_info_books.loc[...]    #\n",
    "missing_info_books[\"description\"] = missing_info_books[\"description\"].fillna(\"\")\n",
    "# Step 2: Create the 'words_in_description' column\n",
    "missing_info_books[\"words_in_description\"] = missing_info_books[\"description\"].apply(lambda x: len(x.split()))\n",
    "# Step 3: Filter books where description has between 1 and 4 words (inclusive)\n",
    "short_descriptions = missing_info_books.loc[\n",
    "    missing_info_books[\"words_in_description\"].between(1, 4),\n",
    "    \"description\"\n",
    "]\n",
    "short_descriptions\n",
    "#bin1\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1ee99080202de8b",
   "metadata": {},
   "source": [
    "short_descriptions = missing_info_books.loc[\n",
    "    missing_info_books[\"words_in_description\"].between(5, 14),\n",
    "    \"description\"\n",
    "]\n",
    "short_descriptions\n",
    "#bin2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c86a48109dd53b53",
   "metadata": {},
   "source": [
    "short_descriptions = missing_info_books.loc[\n",
    "    missing_info_books[\"words_in_description\"].between(15, 24),\n",
    "    \"description\"\n",
    "]\n",
    "short_descriptions\n",
    "#bin3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a56d69bcafe90228",
   "metadata": {},
   "source": [
    "short_descriptions = missing_info_books.loc[\n",
    "    missing_info_books[\"words_in_description\"].between(25, 34),\n",
    "    \"description\"\n",
    "]\n",
    "short_descriptions\n",
    "#bin4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cfaa5603848452bc",
   "metadata": {},
   "source": [
    "#we are at a point where there is enough information,so it should be reasonable to use 25 words and up in the description as cut_off and now we see the impact of that is going to be\n",
    "#create filtering code to clean up the data set again and remove all of those that have less than 25 in the description\n",
    "#so we creaet a new data frame called book missing 25 words  and filter book missing and this will be all of those rows where word in description are more than equal to 25 and keeep those\n",
    "# Ensure you're working on a clean copybut i am using loc to filter row and column\n",
    "missing_info_books = missing_info_books.loc[...]\n",
    "\n",
    "# Remove rows with less than 25 words in the description\n",
    "books_missing_25_words = missing_info_books.loc[\n",
    "    missing_info_books[\"words_in_description\"] >= 25\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a11a7b244ccbbd1",
   "metadata": {},
   "source": [
    "# Shows how many rows are left\n",
    "books_missing_25_words"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a36d5a16d046b2cf",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "#different code works similarly just comment it to compare later on\n",
    "# Make a copy to work safely\n",
    "#books_missing_25_words = books_missing_25_words.copy()\n",
    "\n",
    "# Fill NaNs in subtitle with empty string\n",
    "#books_missing_25_words[\"subtitle\"] = books_missing_25_words[\"subtitle\"].fillna(\"\")\n",
    "\n",
    "# Create new column by combining title and subtitle (only include subtitle if it's non-empty)\n",
    "books_missing_25_words[\"title_and_subtitle\"] = books_missing_25_words.apply(\n",
    "    lambda row: f\"{row['title']}: {row['subtitle']}\" if row[\"subtitle\"].strip() else row[\"title\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Optional: check the result\n",
    "books_missing_25_words\n",
    "\"\"\"\n",
    "#we want to create some final variable so cleaning and screening is completely done\n",
    "#subtitle field:it's unusable on it's own because there is quite a lot of missing value but it's quite common to have a convention of books paired with title and subtitle\n",
    "#so we create a new field called title and subtitle and pair the two if it existalso that where subtitle is missing we add the npw clause and we specify that when\n",
    "# #also that where subtitle is missing we add the npw clause and we specify that when books missing and subtitle is missing use just the title but when it is present that is when there is a sub title what i want to do is to aggregate the title and the sub title\n",
    "#and convert that explicitly to a string because there are lot of missing value and they are consider as a float in pandas we can join them using a colon\n",
    "# Make a copy to work safely but i am using loc to filter row and column\n",
    "missing_info_books = missing_info_books.loc[...]\n",
    "\n",
    "# Create the 'title_and_subtitle' column using np.where\n",
    "books_missing_25_words[\"title_and_subtitle\"] = np.where(\n",
    "    books_missing_25_words[\"subtitle\"].notna() & books_missing_25_words[\"subtitle\"].astype(str).str.strip().ne(\"\"),\n",
    "    books_missing_25_words[\"title\"].astype(str) + \": \" + books_missing_25_words[\"subtitle\"].astype(str),\n",
    "    books_missing_25_words[\"title\"].astype(str)\n",
    ")\n",
    "# Optional: check the result\n",
    "books_missing_25_words\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ed36451f3e60a33",
   "metadata": {},
   "source": [
    "#create a new description column which tags each description with unique identifier i have 2 isbn13 and isbn 10 among which i will perform in isbn13,similar to title and subtitle ,create a new variable called tag description and take isbn 13 and description and do same as what i have done for title and subtitle\n",
    "# Make a safe copy if needed but i am using loc to filter row and column\n",
    "books_missing_25_words = books_missing_25_words.loc[...]\n",
    "\n",
    "# Ensure isbn13 and description are strings, and handle missing description\n",
    "books_missing_25_words[\"description\"] = books_missing_25_words[\"description\"].fillna(\"\")\n",
    "\n",
    "# Create the 'tagged_description' column\n",
    "books_missing_25_words[\"tagged_description\"] = books_missing_25_words[\"isbn13\"].astype(str) + \": \" + books_missing_25_words[\"description\"]\n",
    "\n",
    "# Optional: preview result\n",
    "books_missing_25_words\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3cc39dec70853c09",
   "metadata": {},
   "source": [
    "# Drop the specified columns\n",
    "books_cleaned = books_missing_25_words.drop(columns=[\"missing_desription\", \"age_of_books\", \"subtitle\"])\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "books_cleaned.to_csv(\"books_cleaned.csv\", index=False)\n",
    "\n",
    "# Optional: Check the result by loading and displaying the first few rows\n",
    "books_cleaned\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
